% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mmm_clustering.R
\name{cluster_mmm}
\alias{cluster_mmm}
\title{Mixture Markov Model Clustering for Sequences}
\usage{
cluster_mmm(
  data,
  k,
  max_iter = 300L,
  tol = 1e-06,
  n_starts = 10L,
  seed = NULL,
  verbose = TRUE,
  min_cluster_size = 1L,
  smoothing = 0.1
)
}
\arguments{
\item{data}{A data frame or matrix where rows represent sequences and columns
represent time points. Missing values (NA) are handled appropriately.
Each cell should contain a state label (character or factor).}

\item{k}{An integer specifying the number of mixture components (clusters).
Must be between 2 and the number of sequences minus 1.}

\item{max_iter}{An integer specifying the maximum number of EM iterations
per restart. Default is 300.}

\item{tol}{A numeric value specifying the convergence tolerance for the
log-likelihood change between iterations. Default is 1e-6.}

\item{n_starts}{An integer specifying the number of random restarts to
perform. Multiple restarts help avoid local optima. Default is 10.}

\item{seed}{An integer for random seed setting to ensure reproducibility.
If NULL, uses current random state. Default is NULL.}

\item{verbose}{A logical value indicating whether to print progress
information during fitting. Default is TRUE.}

\item{min_cluster_size}{An integer specifying the minimum number of sequences
required per cluster for a solution to be considered valid. Default is 1.}

\item{smoothing}{A numeric value for Laplace smoothing parameter to prevent
zero probabilities. Default is 0.1.}
}
\value{
A list of class \code{mmm_result} containing:
\describe{
\item{assignments}{Integer vector of cluster assignments for each sequence}
\item{responsibilities}{Matrix of posterior probabilities (n_sequences x k)}
\item{log_likelihood}{Final log-likelihood of the best model}
\item{bic}{Bayesian Information Criterion}
\item{aic}{Akaike Information Criterion}
\item{models}{List of k trained Markov models with named states}
\item{mixture_weights}{Named vector of cluster mixing weights}
\item{k}{Number of clusters}
\item{n_parameters}{Total number of model parameters}
\item{converged}{Logical indicating convergence of best model}
\item{n_iterations}{Number of iterations for best model}
\item{cluster_sizes}{Named vector of cluster sizes}
\item{cluster_proportions}{Named vector of cluster proportions}
\item{avg_posterior_per_cluster}{Named vector of average posterior probabilities per cluster}
\item{avg_posterior_overall}{Overall average posterior probability}
\item{entropy}{Normalized Shannon entropy (0 = perfect separation, 1 = maximum uncertainty)}
\item{states}{Character vector of unique states in data}
\item{call}{The matched call}
}
}
\description{
Fits a mixture of first-order Markov models to sequence data using the
Expectation-Maximization (EM) algorithm. Provides robust estimation through
multiple random restarts and comprehensive model diagnostics.
}
\details{
The function implements a mixture of first-order Markov models where each
component models sequences with initial state probabilities and transition
probabilities between states, along with mixing weights for component membership.

The EM algorithm alternates between E-step (computing posterior probabilities
of cluster membership) and M-step (updating model parameters based on weighted data).

Multiple random restarts are performed to avoid local optima, with the
best solution (highest log-likelihood) selected as the final result.
}
\examples{
\dontrun{
# Create example sequence data
data <- data.frame(
  T1 = c("A", "B", "A", "C", "A", "B"),
  T2 = c("B", "A", "B", "A", "C", "A"),
  T3 = c("C", "C", "A", "B", "B", "C")
)

# Fit MMM with 2 clusters
result <- cluster_mmm(data, k = 2, n_starts = 5, seed = 123)
print(result)

# Access transition matrix for cluster 1
result$models[[1]]$transition
}

}
